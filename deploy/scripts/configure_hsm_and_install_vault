#!/usr/bin/env bash
set +x

function header (){
    echo "======================"
    echo "${1}"
    echo "======================"
}

function footer (){
    echo "======================"
    echo ""
}

function log(){
    echo " "
    echo $(date) - $1
    echo " "
}


log "Setup Variables"
log "####################################CONFIGURE VARIABLES####################################"

read -p "What is your AWS Access Key? [AKIAVXXXX]:" AUTOMATION_USER_AWS_ACCESS_KEY
export AUTOMATION_USER_AWS_ACCESS_KEY=${AUTOMATION_USER_AWS_ACCESS_KEY:-AKIAVXXXX}
echo " "

read -p "What is your AWS Secret Key? [fKBa2QqJXXXX]:" AUTOMATION_USER_AWS_SECRET_KEY
export AUTOMATION_USER_AWS_SECRET_KEY=${AUTOMATION_USER_AWS_SECRET_KEY:-fKBa2QqJXXXX}
echo " "

read -p "What is your AWS region? [us-east-2]:" TF_VAR_aws_region
export TF_VAR_aws_region=${TF_VAR_aws_region:-us-east-2}
echo " "

log "setup aws credential file entry."
check_credential_file=$(cat ~/.aws/credentials | grep 'sifchain-automation')
if [ -z "${check_credential_file}" ]; then
    log "configuring ~/.aws/credentials"
    echo '' >> ~/.aws/credentials
    echo '[sifchain-automation]' >> ~/.aws/credentials
    echo "aws_access_key_id = ${AUTOMATION_USER_AWS_ACCESS_KEY}" >> ~/.aws/credentials
    echo "aws_secret_access_key = ${AUTOMATION_USER_AWS_SECRET_KEY}" >> ~/.aws/credentials
    #echo "region = ${TF_VAR_aws_region}" >> ~/.aws/credentials
else
    log "credential file already configured."
fi
export TF_VAR_cred_profile=sifchain-automation
export TF_VAR_profile=devnet

read -p "What is your Kubernetes Cluster Name? [sifchain-aws-tempnet-us]:" TF_VAR_aws_kubernetes_cluster_name
export TF_VAR_aws_kubernetes_cluster_name=${TF_VAR_aws_kubernetes_cluster_name:-sifchain-aws-tempnet-us}
echo " "

read -p "What is your App Environment? [tempnet]:" TF_VAR_app_env
export TF_VAR_app_env=${TF_VAR_app_env:-tempnet}
echo " "

export TF_VAR_hsm_name=sifchain-${TF_VAR_app_env}-hsm
log "$TF_VAR_hsm_name"

read -p "Vault enterprise License String [02MV4UU43]:" hsm_license_key
export hsm_license_key=${hsm_license_key:-02MV4UU43}
echo " "

export TF_VAR_cluster_subnet_id=$(aws eks describe-cluster --name $TF_VAR_aws_kubernetes_cluster_name --region $TF_VAR_aws_region | jq '.cluster.resourcesVpcConfig.subnetIds' | jq -c '.[1]' | tr -d '"')
log "EKS Cluster Subnet-ID: $TF_VAR_cluster_subnet_id"

export TF_VAR_cluster_security_group_id=$(aws eks describe-cluster --name $TF_VAR_aws_kubernetes_cluster_name --region $TF_VAR_aws_region | jq '.cluster.resourcesVpcConfig.securityGroupIds' | jq -c '.[]' | tr -d '"')
log "EKS Cluster SecurityGroup-ID: $TF_VAR_cluster_security_group_id"

export TF_VAR_cluster_node_security_group_id=$(aws ec2 describe-security-groups --region $TF_VAR_aws_region --filters Name=tag:Name,Values=$TF_VAR_aws_kubernetes_cluster_name-eks_cluster_sg | jq '.SecurityGroups' | jq -c '.[]' | jq '.GroupId' | tr -d '"' | tr -d '\n')
log "EKS Cluster SecurityGroup-ID: $TF_VAR_cluster_node_security_group_id"

export TF_VAR_cluster_level_security_group_id=$(aws ec2 describe-security-groups --region $TF_VAR_aws_region --filters Name=tag:"aws:eks:cluster-name",Values=$TF_VAR_aws_kubernetes_cluster_name | jq '.SecurityGroups' | jq -c '.[]' | jq '.GroupId' | tr -d '"' | tr -d '\n')
log "EKS Node SecurityGroup-ID: $TF_VAR_cluster_level_security_group_id"

export HSM_USER=vault_user
log "HSM Vault User: $HSM_USER"

read -p "HSM Password for Vaults Cloud HSM User? [GHScnsdj523jfai48cjaFJ]:" HSM_PASSWORD
export HSM_PASSWORD=${HSM_PASSWORD:-GHScnsdj523jfai48cjaFJ}
echo " "

read -p "HSM Admin Password for Vaults Cloud HSM User? [nsSDJRwe832Wnfwmle34Dks]:" HSM_ADMIN_PASSWORD
export HSM_ADMIN_PASSWORD=${HSM_ADMIN_PASSWORD:-nsSDJRwe832Wnfwmle34Dks}
echo " "

read -p "Docker Hub Username for Kubernetes Docker Secret for Vault? [gzukel]:" DOCKER_USERNAME
export DOCKER_USERNAME=${DOCKER_USERNAME:-gzukel}
echo " "

read -p "Docker Hub Generated Access Token for Kubernetes Docker Secret for Vault? [930d1007-bdc2-4b32-b0dd-02be51abc03e]:" DOCKER_PASSWORD
export DOCKER_PASSWORD=${DOCKER_PASSWORD:-c354af65-XXX}
echo " "

export DOCKER_SERVER=https://index.docker.io/v2/
log "Docker Server: ${DOCKER_SERVER}"

read -p "Docker Hub Email Associated with Access Token? [grant@sifchain.finance]:" DOCKER_EMAIL
export DOCKER_EMAIL=${DOCKER_EMAIL:-grant@sifchain.finance}
echo " "

read -p "Is IS_DEBUG? [true]:" IS_DEBUG
export IS_DEBUG=${IS_DEBUG:-true}
echo " "

log "Configure AWS environment for Vault HSM automation."
log "###############################################################################"

log "####################################CONFIGURE AWS####################################"

log "#############Create AWS Bucket############"
log "Check if s3 bucket vault-key-backup-${TF_VAR_app_env}  exists."
bucket_check=$(aws s3api list-buckets --region $TF_VAR_aws_region | grep vault-key-backup-$TF_VAR_app_env )
if [ "$IS_DEBUG" == "true" ]; then
    log "key vault backup s3 bucket exist check ${bucket_check}"
fi
if [ -z "${bucket_check}" ]; then
    log "Create S3 Bucket vault-key-backup in private mode."
    aws s3api create-bucket --bucket vault-key-backup-$TF_VAR_app_env --acl private --region $TF_VAR_aws_region --create-bucket-configuration LocationConstraint=$TF_VAR_aws_region
fi
log "#######################################"

log "#############Enable AWS Bucket Encryption############"
log "Check if s3 bucket vault-key-backup-${TF_VAR_app_env} is encrypted."
bucket_check_encrypt=$(aws s3api get-bucket-encryption --region $TF_VAR_aws_region --bucket vault-key-backup-$TF_VAR_app_env | grep AES256)
if [ "$IS_DEBUG" == "true" ]; then
    log "s3 bucket is encrypted check ${bucket_check_encrypt}"
fi
if [ -z "${bucket_check_encrypt}" ]; then
    log "Enable Bucket Encryption"
    aws s3api put-bucket-encryption --bucket vault-key-backup-$TF_VAR_app_env --region $TF_VAR_aws_region --server-side-encryption-configuration '{"Rules": [{"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}]}' > /dev/null
fi
log "#######################################"

#DEPLOY HSM
aws eks update-kubeconfig --name ${TF_VAR_aws_kubernetes_cluster_name} --region $TF_VAR_aws_region

log "Build the HSM terraform file on demand and use the system aws credential file."
echo '
variable "aws_region" {}
variable "cred_profile" {}
variable "cluster_subnet_id" {}
variable "cluster_security_group_id" {}
variable "profile" {}
variable "hsm_name" {}
variable "cluster_level_security_group_id" {}

provider "aws" {
  region     = var.aws_region
  shared_credentials_file = "~/.aws/credentials"
  profile                 = var.cred_profile
}

data "aws_subnet" "selected" {
  id = var.cluster_subnet_id
}

resource "aws_cloudhsm_v2_cluster" "cloudhsm_v2_cluster" {
  hsm_type   = "hsm1.medium"
  subnet_ids = data.aws_subnet.selected.*.id
  tags = {
    Name = var.hsm_name
  }
}

resource "aws_security_group_rule" "allow_all" {
  type              = "ingress"
  to_port           = 2223
  protocol          = "-1"
  from_port         = 2225
  security_group_id = aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster.security_group_id
  source_security_group_id = var.cluster_security_group_id
}

resource "aws_security_group_rule" "allow_all_level" {
  type              = "ingress"
  to_port           = 2223
  protocol          = "-1"
  from_port         = 2225
  security_group_id = aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster.security_group_id
  source_security_group_id = var.cluster_level_security_group_id
}

resource "aws_cloudhsm_v2_hsm" "cloudhsm_v2_hsm" {
  subnet_id  = data.aws_subnet.selected.id
  cluster_id = aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster.cluster_id
  depends_on = ["aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster"]
}


resource "null_resource" "previous" {
    depends_on = ["aws_cloudhsm_v2_hsm.cloudhsm_v2_hsm"]
}

resource "time_sleep" "wait_30_seconds" {
  depends_on = [null_resource.previous]
  create_duration = "60s"
}

resource "null_resource" "echo_cluster_id" {
  provisioner "local-exec" {
    command = "echo \"${aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster.cluster_id}\" > cluster_id"
  }
  depends_on = [time_sleep.wait_30_seconds]
}

resource "null_resource" "get_csr" {
  provisioner "local-exec" {
    command = "aws cloudhsmv2 describe-clusters --region ${var.aws_region} --filters clusterIds=${aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster.cluster_id} --output text --query 'Clusters[].Certificates.ClusterCsr' > ${aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster.cluster_id}_ClusterCsr.csr"
  }
  depends_on = [time_sleep.wait_30_seconds]
}

resource "null_resource" "generate_key" {
  provisioner "local-exec" {
    command = "openssl genrsa -aes256 -out customerCA.key 2048"
  }
  depends_on = ["null_resource.get_csr"]
}

resource "null_resource" "generate_csr" {
  provisioner "local-exec" {
    command = "openssl req -new -x509 -days 3652 -key customerCA.key -out customerCA.crt -subj '/C=US/ST=California/O=Sifchain/OU=chainOps/L=SanJose/CN=HSM'"
  }
  depends_on = ["null_resource.generate_key"]
}

resource "null_resource" "generate_cert" {
  provisioner "local-exec" {
    command = "openssl x509 -req -days 3652 -in ${aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster.cluster_id}_ClusterCsr.csr -CA customerCA.crt -CAkey customerCA.key -CAcreateserial -out ${aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster.cluster_id}_CustomerHsmCertificate.crt"
  }
  depends_on = ["null_resource.generate_csr"]
}

resource "null_resource" "initialize_cluster" {
  provisioner "local-exec" {
    command = "aws cloudhsmv2 initialize-cluster --region ${var.aws_region} --cluster-id ${aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster.cluster_id} --signed-cert file://${aws_cloudhsm_v2_cluster.cloudhsm_v2_cluster.cluster_id}_CustomerHsmCertificate.crt --trust-anchor file://customerCA.crt"
  }
  depends_on = ["null_resource.generate_cert"]
}
' > main.tf

if [ "$IS_DEBUG" == "true" ]; then
    log ""
    log "Output main.tf for review"
    cat main.tf
fi

log ""
log "Terraform Init."
terraform init .

log ""
log "Apply Terraform File."
terraform apply .

log " "
log " "
log "#####################RETRIEVE HSM CLUSTER INFORMATION#####################"

log "Get the HSM Cluster ID from ClusterID File."
export hsm_cluster_id=$(cat cluster_id | tr -d '\n')
if [ "$IS_DEBUG" == "true" ]; then
    log "HSM Cluster ID: ${hsm_cluster_id}"
fi

log "################################Wait for HSM to be Initialized#################################"
log "Wait for the cluster to be in initalized state.."
max_loops=100
current_loop=0
while true
do
    check_init=$(aws cloudhsmv2 describe-clusters --region $TF_VAR_aws_region --filters clusterIds=$hsm_cluster_id | grep 'INITIALIZED')
    log "Is Cluster Initialized: ${check_init}"
    if [ -z "${check_init}" ]; then
        current_loop=$((current_loop+1))
        if [ "${current_loop}" == "${max_loops}" ]; then
            log "hit max loop count."
            exit 1
        fi
        log "${current_loop} of ${max_loops}... sleeping for 5 seconds."
        sleep 10
    else
        check_is_not_initalizing=$(aws cloudhsmv2 describe-clusters --region $TF_VAR_aws_region --filters clusterIds=$hsm_cluster_id | grep 'INITIALIZE_IN_PROGRESS')
        if [ -z "${check_is_not_initalizing}" ]; then
            log "Cluster initalized. Continuing."
            break
        fi
        log "Still initializing"
        current_loop=$((current_loop+1))
        if [ "${current_loop}" == "${max_loops}" ]; then
            log "hit max loop count."
            exit 1
        fi
        log "${current_loop} of ${max_loops}... sleeping for 5 seconds."
        sleep 10
    fi
done

log "Get the CLOUD HSM IP for your newly created cloudHSM cluster."
export HSM_IP=$(aws cloudhsmv2 describe-clusters --region $TF_VAR_aws_region --filters clusterIds=$hsm_cluster_id | jq '.Clusters' | jq -c '.[0]' | jq -c '.Hsms' | jq -c '.[0]' | jq '.EniIp' | tr -d '"')
if [ "$IS_DEBUG" == "true" ]; then
    log "CloudHSM IP: ${HSM_IP}"
fi

if [ -z "HSM_IP" ]; then
    echo "Cloud HSM IP ${HSM_IP}"
    exit 1
fi

log "Get the contents of the generated CA certificate for the CloudHSM.."
CA_CONTENTS=$(cat customerCA.crt | base64 | tr -d '\n')

log "#######################################"
log " "
log "#####################USE KUBERENETES TEMP SHELL TO CONFIGURE HSM USERS#####################"
log "Open temporary kubernetes shell to configure the cloud hsm users. This is the only manual part please pay attention to the directed copy and paste."
kubectl run test-shell --rm -i --tty --image ubuntu:bionic-20210512 -- bash -c "\
    apt-get update -y && \
    echo 'ca contents' && \
    echo $CA_CONTENTS && \
    apt-get install nano wget unzip ca-certificates gnupg openssl libpcap-dev dumb-init tzdata -y && \
    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \
    bash Miniconda3-latest-Linux-x86_64.sh -ab && \
    export PATH=~/miniconda3/bin:${PATH} && \
    wget https://s3.amazonaws.com/cloudhsmv2-software/CloudHsmClient/Bionic/cloudhsm-client_latest_u18.04_amd64.deb && \
    wget https://s3.amazonaws.com/cloudhsmv2-software/CloudHsmClient/Bionic/cloudhsm-client-pkcs11_latest_u18.04_amd64.deb && \
    apt install ./cloudhsm-client_latest_u18.04_amd64.deb -y && \
    apt install ./cloudhsm-client-pkcs11_latest_u18.04_amd64.deb -y && \
    echo \"${CA_CONTENTS}\" | base64 --decode > /opt/cloudhsm/etc/customerCA.crt && \
    apt-get install opensc -y && \
    /opt/cloudhsm/bin/configure -a ${HSM_IP} && \
    echo 'You will run the following cli commands in the aws-cloudhsm cli. This will configure the users on aws.' && \
    echo 'There is no way to automate this so you must run this manually.' && \
    echo 'aws-cloudhsm> loginHSM PRECO admin password' && \
    echo 'aws-cloudhsm> changePswd PRECO admin {HSM_ADMIN_PASSWORD}' && \
    echo 'aws-cloudhsm> logoutHSM' && \
    echo 'aws-cloudhsm> loginHSM CO admin {HSM_ADMIN_PASSWORD}' && \
    echo 'aws-cloudhsm> createUser CU {HSM_USER} {HSM_PASSWORD}' && \
    echo 'aws-cloudhsm> quit' && \
    export HSM_USER=vault_user && \
    echo HSM_USER: $HSM_USER && \
    echo HSM_PASSWORD: $HSM_PASSWORD && \
    echo HSM_ADMIN_PASSWORD: $HSM_ADMIN_PASSWORD && \
    /opt/cloudhsm/bin/cloudhsm_mgmt_util /opt/cloudhsm/etc/cloudhsm_mgmt_util.cfg && \
    exit"

log "#######################################"
log ""
log "#####################DEPLOY CUSTOM VAULT ENT IMAGE WITH HELM#####################"
log "Generate Helm Values file for Vault"
echo "
global:
  imagePullSecrets:
    - name: vault-docker-secret

server:
  image:
    repository: sifchain/vault
    pullPolicy: Always
    tag: \"1.2.7\"

  agentImage:
    repository: sifchain/vault
    tag: \"1.2.7\"
    pullPolicy: Always

  extraSecretEnvironmentVars:
    - envName: AWS_ACCESS_KEY_ID
      secretName: vault-eks-creds
      secretKey: AWS_ACCESS_KEY_ID

    - envName: AWS_SECRET_ACCESS_KEY
      secretName: vault-eks-creds
      secretKey: AWS_SECRET_ACCESS_KEY

    - envName: CLOUD_HSM_IP
      secretName: vault-config-secrets
      secretKey: CLOUD_HSM_IP

    - envName: CUSTOMER_CA
      secretName: vault-config-secrets
      secretKey: CUSTOMER_CA

    - envName: VAULT_ENT_LICENSE
      secretName: vault-config-secrets
      secretKey: VAULT_ENT_LICENSE

  resources:
    requests:
      memory: 256Mi
      cpu: 100m

    limits:
      memory: 512Mi
      cpu: 500m

  extraVolumes:
    - type: secret
      name: vault-eks-creds

  readinessProbe:
    enabled: true
    path: \"/v1/sys/health?standbyok=true&sealedcode=204&uninitcode=204&perfstandbyok=true\"
    initialDelaySeconds: 100

  livenessProbe:
    enabled: true
    path: \"/v1/sys/health?standbyok=true&perfstandbyok=true\"
    initialDelaySeconds: 100

  auditStorage:
    enabled: true

  dataStorage:
    enabled: true

  standalone:
    enabled: false

  ha:
    enabled: true
    replicas: 3

    raft:
      enabled: true
      setNodeId: true

      config: |
        listener \"tcp\" {
          tls_disable = 1
          address = \"[::]:8200\"
          cluster_address = \"[::]:8201\"
        }

        storage \"raft\" {
          path = \"/vault/data\"
          retry_join {
            leader_api_addr = \"http://vault-0.vault-internal.vault:8200\"
          }
          retry_join {
            leader_api_addr = \"http://vault-1.vault-internal.vault:8200\"
          }
          retry_join {
            leader_api_addr = \"http://vault-2.vault-internal.vault:8200\"
          }
        }

        seal "pkcs11" {
          lib            = \"/opt/cloudhsm/lib/libcloudhsm_pkcs11.so\"
          slot           = "1"
          pin            = \"${HSM_USER}:${HSM_PASSWORD}\"
          generate_key   = \"true\"
          key_label      = \"vault\"
          hmac_key_label = \"vault\"
        }

        #disable_mlock    = \"False\"
        ui               = \"True\"

        service_registration \"kubernetes\" {
          namespace      = \"vault\"
          pod_name       = \"vault\"
        }

ui:
  enabled: true
  serviceType: \"ClusterIP\"
  serviceNodePort: null
  externalPort: 8200
  activeVaultPodOnly: true

" > hsm_values.yaml

cat hsm_values.yaml

log "Generate Vault Secret Yaml."
echo "
---
apiVersion: v1
stringData:
  CLOUD_HSM_IP: \"${HSM_IP}\"
  CUSTOMER_CA: \"${CA_CONTENTS}\"
  VAULT_ENT_LICENSE: \"${hsm_license_key}\"
kind: Secret
metadata:
  name: vault-config-secrets
  namespace: vault
type: Opaque

---
apiVersion: v1
stringData:
  AWS_ACCESS_KEY_ID: \"${AUTOMATION_USER_AWS_ACCESS_KEY}\"
  AWS_SECRET_ACCESS_KEY: \"${AUTOMATION_USER_AWS_SECRET_KEY}\"
kind: Secret
metadata:
  name: vault-eks-creds
  namespace: vault
type: Opaque
" > hsm_secret.yaml

log " Create kubernetes namespace if it doesn't exist."
kubectl create namespace vault

log "Create Kubernetes Docker Secret."
kubectl create secret docker-registry vault-docker-secret --docker-server="https://index.docker.io/v2/" --docker-username="${DOCKER_USERNAME}" --docker-password="${DOCKER_PASSWORD}" --docker-email="${DOCKER_EMAIL}" -n vault

log "Apply the Kubernetes HSM Secret."
kubectl apply -f hsm_secret.yaml -n vault

log "Add the Hashicorp repo if it doesn't exist."
helm repo add hashicorp https://helm.releases.hashicorp.com

log "HELM unstall our custom ent vault deployment with hsm_vaules.yaml"

helm upgrade --install vault hashicorp/vault --namespace vault -f hsm_values.yaml --set image.repository=sifchain/vault --set image.tag=1.2.6


if [ "$IS_DEBUG" == "true" ]; then
    log "Output Kubernetes Logs for Vault 0 Pod"
    kubectl logs -n vault vault-0
fi

log "Rest for 30 seconds just to give things some time to start up. This Gives kubernetes time to schedule the new pod."
sleep 30

log "################################INIT VAULT#################################"
log "Start watch loop and wait for the vault-0 pod to be running with 1/1 status in order to init properly."
max_loops=100
current_loop=0
while true
do
    check_vault_zero_is_running=$(kubectl get pods -n vault vault-0 | grep '1/1')
    log "Vault running status: ${check_vault_zero_is_running}"
    if [ -z "${check_vault_zero_is_running}" ]; then
        log "Vault Pod is not in 1 of 1 state: ${check_vault_zero_is_running}"
    else
        log "pod running lets try to init until success."
        sleep 20
        max_loops=100
        current_loop=0
        log "trying to vault init."
        kubectl exec -it vault-0 -n vault -- vault operator init > vault_output
        while [ $? -ne 0 ]; do
            sleep 10
            current_loop=$((current_loop+1))
            if [ "${current_loop}" == "${max_loops}" ]; then
                log "hit max loop count."
                exit 1
            fi
            kubectl exec -it vault-0 -n vault -- vault operator init > vault_output
        done
        break
    fi
    current_loop=$((current_loop+1))
    if [ "${current_loop}" == "${max_loops}" ]; then
        log "hit max loop count."
        exit 1
    fi
    log "${current_loop} of ${max_loops}... sleeping for 5 seconds."
    sleep 5
done

log "################################LOGIN VAULT#################################"
log "Initalize Vault, This will Produce a Token output. We will save this to variable and use it to authenticate the vault pod."

vault_output=$(cat vault_output)
VAULT_TOKEN=`echo -n $vault_output | cut -d ':' -f 7 | cut -d ' ' -f 2 | tr -d '\n'`
VAULT_TOKEN=`tr -dc '[[:print:]]' <<< "$VAULT_TOKEN"`
VAULT_TOKEN=`echo -n $VAULT_TOKEN | rev | cut -c4- | rev`
max_loops=100
current_loop=0
kubectl exec -n vault -it vault-0 -- vault login $VAULT_TOKEN
while [ $? -ne 0 ]; do
    sleep 10
    log "Trying to login to vault with token from file vault_output"
    vault_output=$(cat vault_output)
    VAULT_TOKEN=`echo -n $vault_output | cut -d ':' -f 7 | cut -d ' ' -f 2 | tr -d '\n'`
    VAULT_TOKEN=`tr -dc '[[:print:]]' <<< "$VAULT_TOKEN"`
    VAULT_TOKEN=`echo -n $VAULT_TOKEN | rev | cut -c4- | rev`
    current_loop=$((current_loop+1))
    if [ "${current_loop}" == "${max_loops}" ]; then
        log "hit max loop count."
        exit 1
    fi
    kubectl exec -n vault -it vault-0 -- vault login $VAULT_TOKEN
done
log "#####################################################"

log "################################Enable Enterprise Vault License#################################"
kubectl exec -n vault  vault-0 -- vault write sys/license text=@/opt/cloudhsm/etc/sifchain-vault-ent-license.hcli
log "#####################################################"

log "################################Enable KV-V2 Secrets Engine#################################"
log "enable kv-v2 secrets engine."
kubectl exec -n vault  vault-0 -- vault secrets enable kv-v2
log "#####################################################"

log "################################BACKUP TOKENS IN ENCRYPTED SECURE S3 Bucket vault-key-backup-$TF_VAR_profile#################################"
log "Upload vault init token backup so s3 that only the vault automation user has access too."
aws s3 cp ./vault_output s3://vault-key-backup-$TF_VAR_app_env/vault-master-keys.backup --region $TF_VAR_aws_region
log "#####################################################"
